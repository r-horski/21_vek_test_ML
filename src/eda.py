import sys
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import f1_score
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier


def read_data(train_path="../data/train.csv", test_path="../data/test.csv", sample_sub_path="../submissions/sample_submission.csv"):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç train, test –∏ sample_submission"""
    train = pd.read_csv(train_path)
    test = pd.read_csv(test_path)
    sample = pd.read_csv(sample_sub_path)
    print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    return train, test, sample


def basic_info(df, name):
    """–í—ã–≤–æ–¥–∏—Ç –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ"""
    print(f"\nüìä –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {name}")
    print(f"–†–∞–∑–º–µ—Ä: {df.shape}")
    print(f"–ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
    print(df.head())


def quality_checks(df, name, id_col="id"):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ–ø—É—Å–∫–∏, –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
    print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {name}")
    miss = df.isna().sum().sort_values(ascending=False)
    print("–ü—Ä–æ–ø—É—Å–∫–∏ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º:")
    print(miss[miss > 0])
    dup_rows = df.duplicated().sum()
    print(f"–î—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Ç—Ä–æ–∫: {dup_rows}")
    if id_col in df.columns:
        dup_ids = df[id_col].duplicated().sum()
        print(f"–î—É–±–ª–∏–∫–∞—Ç–æ–≤ id: {dup_ids}")


def plot_target_distribution(train):
    """–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ–∏–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"""
    if "y" not in train.columns:
        print("‚ö†Ô∏è –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'y' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
        return
    counts = train["y"].value_counts()
    print("\nüìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:")
    print(counts)


def time_range_check(train, test):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π relative_date_number"""
    if "relative_date_number" not in train.columns or "relative_date_number" not in test.columns:
        print("‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ relative_date_number –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
        return
    print("\n‚è≥ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –≤—Ä–µ–º–µ–Ω–∏:")
    print(f"Train: {train['relative_date_number'].min()} ‚Üí {train['relative_date_number'].max()}")
    print(f"Test: {test['relative_date_number'].min()} ‚Üí {test['relative_date_number'].max()}")


def correlation_analysis(train):
    """–°—á–∏—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"""
    if "y" not in train.columns:
        print("‚ö†Ô∏è –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'y' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
        return
    num_cols = [c for c in train.columns if c.startswith("x")]
    print("\nüîó –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å y:")
    for c in num_cols:
        if train[c].nunique() > 1:
            rho = stats.spearmanr(train[c], train["y"]).correlation
            print(f"{c}: {rho:.3f}")


def time_aware_split(train, n_splits=5):
    """–°–æ–∑–¥–∞—ë—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–ø–ª–∏—Ç—ã –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    if "relative_date_number" not in train.columns:
        print("‚ö†Ô∏è –ù–µ—Ç relative_date_number –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Å–ø–ª–∏—Ç–∞")
        return []
    train_sorted = train.sort_values("relative_date_number").reset_index(drop=True)
    tss = TimeSeriesSplit(n_splits=n_splits)
    splits = []
    for tr_idx, val_idx in tss.split(train_sorted):
        splits.append((train_sorted.index[tr_idx].values, train_sorted.index[val_idx].values))
    print(f"\nüìÇ –°–æ–∑–¥–∞–Ω–æ {len(splits)} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–ø–ª–∏—Ç–æ–≤")
    return splits


def baseline_models(train, splits):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –≤—ã–≤–æ–¥–∏—Ç F1"""
    if not splits or "y" not in train.columns:
        print("‚ö†Ô∏è –ù–µ—Ç —Å–ø–ª–∏—Ç–æ–≤ –∏–ª–∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π")
        return
    features = [c for c in train.columns if c.startswith("x")] + ["g1", "g2", "relative_date_number"]
    X = train[features].copy()
    y = train["y"].values
    for col in ["g1", "g2"]:
        if col in X.columns and not np.issubdtype(X[col].dtype, np.number):
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col].astype(str))
    X = X.fillna(X.median(numeric_only=True))

    for name, model in [
        ("RandomForest", RandomForestClassifier(n_estimators=200, n_jobs=-1, class_weight="balanced")),
        ("LightGBM", LGBMClassifier(n_estimators=300, learning_rate=0.05, class_weight="balanced"))
    ]:
        scores = []
        for tr_idx, val_idx in splits:
            X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]
            X_val, y_val = X.iloc[val_idx], y[val_idx]
            model.fit(X_tr, y_tr)
            preds = model.predict(X_val)
            f1 = f1_score(y_val, preds)
            scores.append(f1)
        print(f"\n‚ö° –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å {name}: —Å—Ä–µ–¥–Ω–∏–π F1 = {np.mean(scores):.4f}")


def main():
    train, test, sample = read_data()
    basic_info(train, "train")
    basic_info(test, "test")
    basic_info(sample, "sample_submission")
    quality_checks(train, "train")
    quality_checks(test, "test")
    plot_target_distribution(train)
    time_range_check(train, test)
    correlation_analysis(train)
    splits = time_aware_split(train, n_splits=5)
    baseline_models(train, splits)
    print("\n‚úÖ EDA –∑–∞–≤–µ—Ä—à—ë–Ω. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ outputs")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è EDA: {e}", file=sys.stderr)
        sys.exit(1)
